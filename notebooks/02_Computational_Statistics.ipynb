{"cells":[{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'book_format'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-e97efbffd281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../code'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbook_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mbook_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../code'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'book_format'"]}],"source":"import sys\nsys.path.insert(0,'../code')\nimport book_format\nbook_format.load_style('../code')"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"import numpy as np"},{"cell_type":"markdown","metadata":{},"source":"# Computational Statistics\n\n## Distributions\n\nIn statistics a <span>**distribution**</span> is a set of values and\ntheir corresponding probabilities.\n\nFor example, if you roll a six-sided die, the set of possible values is\nthe numbers 1 to 6, and the probability associated with each value is\n1/6.\n\nAs another example, you might be interested in how many times each word\nappears in common English usage. You could build a distribution that\nincludes each word and how many times it appears.\n\nTo represent a distribution, one could use two numpy arrays that\nstore values and their probabililties. These two arrays represent a \n<span>**probability mass function**</span> (PMF), which is a way to\nrepresent a distribution mathematically.\n\nTo use numpy arrays the corresponding module has to be imported first:"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"import numpy as np"},{"cell_type":"markdown","metadata":{},"source":["The following code builds two arrays to represent the distribution of\n","outcomes for a six-sided die as a PMF:"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[1. 2. 3. 4. 5. 6.]\n[0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667]\n"}],"source":"vals=np.array([1,2,3,4,5,6],dtype=float)\nprobs = np.full_like(vals,1/6,dtype=float)\n\nprint(vals)\nprint(probs)"},{"cell_type":"markdown","metadata":{},"source":["The array `vals` contains the numbers on the sides of a six-sided die and the array `probs` contains the probabilities  associated with each value $1/6$.\n","\n","Here’s another example that counts the number of times each word appears in a sequence:"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"['bye' 'football' 'hi' 'sky' 'the']\n[1 1 2 1 1]\n"}],"source":"word_list = np.array(['hi', 'the', 'bye', 'hi', 'football', 'sky'])\nwords_unique, words_counts = np.unique(word_list, return_counts=True)\n\nprint(words_unique)\nprint(words_counts)"},{"cell_type":"markdown","metadata":{},"source":["`np.unique` returns the unique values of an array and their counts.\n","The word counts are proportional to the probabilities to find each \n","word in the word list. So after we count all the words, we can compute\n","probabilities by dividing through by the total number of words:"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"['bye' 'football' 'hi' 'sky' 'the']\n[0.16666667 0.16666667 0.33333333 0.16666667 0.16666667]\n"}],"source":"words_probs=words_counts / words_counts.sum()\n\nprint(words_unique)\nprint(words_probs)"},{"cell_type":"markdown","metadata":{},"source":["Once values and prbabilities are defined, one can get the probability associated with any value:"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[0.16666667]\n"}],"source":"prob=words_probs[words_unique=='the']\n\nprint(prob)"},{"cell_type":"markdown","metadata":{},"source":["And that would print the frequency of the word “the” as a fraction of\n","the words in the list.\n","\n","To define a PMF two arrays with the values and their\n","probabilities are used, so the values in the PMF can be any hashable type. The\n","probabilities can be any numerical type, but they are usually\n","floating-point numbers (type `float`)."]},{"cell_type":"markdown","metadata":{},"source":["## The cookie problem\n","\n","In the context of Bayes’s theorem, it is natural to use a Pmf to map\n","from each hypothesis to its probability. In the cookie problem, the\n","hypotheses are $B_1$ and $B_2$. In Python, I represent them with\n","strings:"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":"prior = np.array([0.5,0.5])"},{"cell_type":"markdown","metadata":{},"source":["This distribution, which contains the priors for each hypothesis, is\n","called (wait for it) the <span>**prior distribution**</span>.\n","\n","To update the distribution based on new data (the vanilla cookie), we\n","multiply each prior by the corresponding likelihood. The likelihood of\n","drawing a vanilla cookie from Bowl 1 is 3/4. The likelihood for Bowl 2\n","is 1/2."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":"likelihood = np.array([30/40, 20/40])\nproduct=prior*likelihood"},{"cell_type":"markdown","metadata":{},"source":["`Mult` does what you would expect. It gets the probability for the given\n","hypothesis and multiplies by the given likelihood.\n","\n","After this update, the distribution is no longer normalized, but because\n","these hypotheses are mutually exclusive and collectively exhaustive, we\n","can <span>**renormalize**</span>:"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"norm_constant = product.sum()\nposterior = product / norm_constant"},{"cell_type":"markdown","metadata":{},"source":["The result is a distribution that contains the posterior probability for\n","each hypothesis, which is called (wait now) the <span>**posterior\n","distribution**</span>.\n","\n","Finally, we can get the posterior probability for Bowl 1:"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"The probability of the vanilla cookie came from the first bowl is 0.6\n"}],"source":"print(f\"The probability of the vanilla cookie came from the first bowl is {posterior[0]}\")"},{"cell_type":"markdown","metadata":{},"source":["## The Bayesian framework\n","\n","The Bayesian framework introduced in the original is not used here \n","and all calculations are performed using numpy arrays."]},{"cell_type":"markdown","metadata":{},"source":["## The Monty Hall problem\n","\n","To solve the Monty Hall problem, the hypotheses and the prior are defined first:"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":"hypos = np.array(['A','B','C'])\nprior = np.array([1/3]*3)"},{"cell_type":"markdown","metadata":{},"source":["Following the explanations from the previous chapter the likelihood after choosing the door B is defined as:"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":"likelihood = np.array([0.5, 0, 1])"},{"cell_type":"markdown","metadata":{},"source":["Finally, an update is performed and the posterior is calculated:"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"The probabilities of the car beeing behind doors ['A' 'B' 'C'] are [0.33333333 0.         0.66666667]\n"}],"source":"product = prior*likelihood\nnorm_constant = product.sum()\nposterior = product / norm_constant\nprint(f\"The probabilities of the car beeing behind doors {hypos} are {posterior}\")"},{"cell_type":"markdown","metadata":{},"source":"## Encapsulating the framework\n\nThe Bayesian framework introduced in the original is not used here and all calculations are performed using numpy arrays."},{"cell_type":"markdown","metadata":{},"source":"## The <span>M&M</span> problem\n\nWe can solve the <span>M&M</span> problem using the approach above.\nWriting the `Likelihood` function is tricky, but everything else is\nstraightforward."},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":"mix94_vals=np.array(['brown','yellow','red','green','orange','tan'])\nmix94_counts=np.array([30,20,20,10,10,10],dtype=float)\nmix94_probs=mix94_counts/mix94_counts.sum()\n\nmix96_vals=np.array(['blue','green','orange','yellow','red','brown'])\nmix96_counts=np.array([24,20,16,14,13,13],dtype=float)\nmix96_probs=mix96_counts/mix96_counts.sum()"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"The array `hypos` represents the hypotheses. The first elment of the array `hypos` is that Bag 1 is from 1994 and Bag 2 from 1996. The second element of `hypos` is the other way around."},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":"hypos=['Bag1_94_Bag2_96','Bag1_96_Bag2_94']"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"The prior for both of the hypotheses is 0.5."},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":"prior = np.array([0.5,0.5])"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"The data that we have is an M&M drawn from the first bag was yellow and from the second bag was green. Using this data the likelihood of this even can be calculated for both  of the hypotheses."},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":"likelihood=np.array([mix94_probs[np.argwhere(mix94_vals=='yellow')][0][0]*mix96_probs[np.argwhere(mix96_vals=='green')][0][0],\n                     mix94_probs[np.argwhere(mix94_vals=='green')][0][0] *mix96_probs[np.argwhere(mix96_vals=='yellow')][0][0] ])"},{"cell_type":"markdown","metadata":{},"source":"And finally the update:"},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"The probabilities of the hypothses ['Bag1_94_Bag2_96', 'Bag1_96_Bag2_94'] are [0.74074074 0.25925926]\n"}],"source":"product = prior*likelihood\nnorm_constant = product.sum()\nposterior = product / norm_constant\nprint(f\"The probabilities of the hypothses {hypos} are {posterior}\")"},{"cell_type":"markdown","metadata":{},"source":"The posterior probability of `Bag1 94 Bag2 96` is approximately $20/27$, which is what\nwe got before."},{"cell_type":"markdown","metadata":{},"source":"## Discussion\n\nThis chapter presents a computational approach for performing the Bayesian \nupdate using `numpy`. Implementig all calculations manually is a tedious task\nif you do some realworld analysis and in this case one should go for some \nexisting frameworks for Bayesian analysis. However, performing the calculations \nfrom scratch can be helpful for a better understanding of the analysis procedure.\n\nMost of the examples in the following chapters have the same pattern;\nfor each problem we define a hypotheses set and the corresponding priors. \nThen we calculate the likelihood given some data and finally perform an update\nto get the posterior."},{"cell_type":"markdown","metadata":{},"source":"## Exercises\n\nIn Section [framework] I said that the solution to the cookie problem\ngeneralizes to the case where we draw multiple cookies with replacement.\n\nBut in the more likely scenario where we eat the cookies we draw, the\nlikelihood of each draw depends on the previous draws.\n\nModify the solution in this chapter to handle selection without\nreplacement. Hint: add instance variables to <span>Cookie</span> to\nrepresent the hypothetical state of the bowls, and modify\n<span>Likelihood</span> accordingly. You might want to define a\n<span>Bowl</span> object."}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}